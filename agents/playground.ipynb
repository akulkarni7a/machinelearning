{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.lmstudio import LMStudio\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "llm = LMStudio(\n",
    "    model_name=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q2_K.gguf\",\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sample Tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypotheses(problem:str) -> str:\n",
    "    \"\"\"Given a problem and some context, generate a few hypotheses for what the solution is\"\"\"\n",
    "    prompt = \"\"\"\n",
    "        You are an expert at problem solving. You take in a problem and you come up with a few hypotheses for what is causing the problem. \n",
    "        For example you might be given:\n",
    "            Problem: I can't log into my email\n",
    "            Hypotheses: ['Password was recently changed.' 'Wifi might not be connected']\n",
    "        \n",
    "        Now, for the new problem.\n",
    "        Problem:\n",
    "    \"\"\"\n",
    "    results = llm.stream_complete(prompt)\n",
    "    return results\n",
    "\n",
    "hypotheses_tool = FunctionTool.from_defaults(fn=generate_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools([multiply_tool,hypotheses_tool], llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current question is in English, and I need to use a tool or multiple tools to help me answer the question. However, at this point, I don't have enough information from the user or the context of the conversation to determine which tool(s) would be best to use.\n",
      "\n",
      "Therefore, I will first generate some hypotheses about what might have caused the car to break down based on general knowledge and common car-related problems. Then, if the hypotheses are not sufficient to answer the question, I may need to request more context from the user or explore specific tools further.\n",
      "Action: generate_hypotheses\n",
      "Action Input: {'problem': 'Why did my car break down?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: <generator object llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict.<locals>.wrapped_gen at 0x3036ced50>\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I received an error message from the `generate_hypotheses` tool. It appears that the generated hypotheses could not be parsed correctly. Let me check the generated hypotheses and ensure they are in the correct format for the assistant to understand before providing them as a potential answer.\n",
      "Action: generate_hypotheses\n",
      "Action Input: {'problem': 'Why did my car break down?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: <generator object llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict.<locals>.wrapped_gen at 0x3036cf0d0>\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I received an error message from the `generate_hypotheses` tool again. Let me double-check that the generated hypotheses are in a format that can be easily understood and used by the assistant as a potential answer.\n",
      "Action: generate_hypotheses\n",
      "Action Input: {'problem': 'Why did my car break down?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: <generator object llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict.<locals>.wrapped_gen at 0x3036cf450>\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I have received an error message three times now from the `generate_hypotheses` tool. It seems that there is a problem with generating hypotheses for this particular question, and I need to find another way to answer it or request more context from the user to proceed further.\n",
      "Answer: I'm unable to definitively determine why your car broke down without more information. Could you please provide some context, such as the make, model, symptoms, or any error codes you may have seen on the dashboard? This will help narrow down the possibilities and potentially give a more accurate answer.\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response=\"I'm unable to definitively determine why your car broke down without more information. Could you please provide some context, such as the make, model, symptoms, or any error codes you may have seen on the dashboard? This will help narrow down the possibilities and potentially give a more accurate answer.\", sources=[ToolOutput(content='<generator object llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict.<locals>.wrapped_gen at 0x3036ced50>', tool_name='generate_hypotheses', raw_input={'args': (), 'kwargs': {'problem': 'Why did my car break down?'}}, raw_output=<generator object llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict.<locals>.wrapped_gen at 0x3036ced50>, is_error=False), ToolOutput(content='Error: Could not parse output. Please follow the thought-action-input format. Try again.', tool_name='unknown', raw_input={}, raw_output='Error: Could not parse output. Please follow the thought-action-input format. Try again.', is_error=False), ToolOutput(content='<generator object llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict.<locals>.wrapped_gen at 0x3036cf0d0>', tool_name='generate_hypotheses', raw_input={'args': (), 'kwargs': {'problem': 'Why did my car break down?'}}, raw_output=<generator object llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict.<locals>.wrapped_gen at 0x3036cf0d0>, is_error=False), ToolOutput(content='<generator object llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict.<locals>.wrapped_gen at 0x3036cf450>', tool_name='generate_hypotheses', raw_input={'args': (), 'kwargs': {'problem': 'Why did my car break down?'}}, raw_output=<generator object llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict.<locals>.wrapped_gen at 0x3036cf450>, is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat(\"Why did my car break down?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
